# configs/config.yaml

# -----------------------------------------------------------------------------
# General Application Settings
# -----------------------------------------------------------------------------
app:
  # The timezone for the application, used for logging and artifact creation.
  # See https://en.wikipedia.org/wiki/List_of_tz_database_time_zones
  timezone: "Asia/Seoul"

  # A file used to identify the project root directory.
  root_marker: "pyproject.toml"

  # The default profile to use if the ENV_PROFILE environment variable is not set.
  default_profile: "high_gpu"

  # Directory names for storing data and logs, relative to the project root.
  directories:
    logs: "logs"
    data: "data"

# -----------------------------------------------------------------------------
# Environment Profiles
# - Define different configurations for various environments (e.g., CPU vs GPU).
# - The active profile is selected via the `ENV_PROFILE` environment variable.
# -----------------------------------------------------------------------------
profiles:
  default_cpu:
    description: "External API and lightweight local models"
    llm_provider: "openai"
    llm_model: "gpt-4o"
    embedding_provider: "openai"
    embedding_model: "text-embedding-3-large"
    # --- NEW SECTIONS START ---
    vision_provider: "huggingface"
    vision_model: "llava-hf/llava-1.5-7b-hf" # Example lightweight vision model
    asr_provider: "huggingface"
    asr_model: "openai/whisper-tiny" # Example lightweight ASR model
    # --- NEW SECTIONS END ---
    icon: ""

  high_gpu:
    description: "Local GPU with powerful models"
    llm_provider: "ollama"
    llm_model: "gemma3:4b-it-qat"
    embedding_provider: "huggingface"
    embedding_model: "jhgan/ko-sroberta-multitask"
    # --- NEW SECTIONS START ---
    vision_provider: "huggingface"
    vision_model: "llava-hf/llava-v1.6-34b-hf" # Your target powerful model
    asr_provider: "huggingface"
    asr_model: "Upstage/phi-3-multimodal-instruct-ko-asr" # Your target powerful model
    # --- NEW SECTIONS END ---
    icon: ""

# -----------------------------------------------------------------------------
# LLM Providers & Models
# - A list of available models for each provider, used to populate the UI.
# -----------------------------------------------------------------------------
llm_providers:
  openai:
    - "gpt-4o"
    - "gpt-4-turbo"
    - "gpt-3.5-turbo"
  ollama:
    - "llama3.1:8b"
    - "gemma3:4b"
    - "gemma3:4b-it-qat"
    - "gpt-oss:20b"
  upstage:
    - "solar-1-mini-chat"

# -----------------------------------------------------------------------------
# Data Ingestion Settings
# -----------------------------------------------------------------------------
ingestion:
  # Parser to use for document processing. Options: "local", "unstructured", "api".
  parser: "local"

  # Configuration for the Upstage Document Parse API loader.
  api_loader:
    # `split`: "none", "element", or "page".
    split: "page"

  # Configuration for splitting documents into chunks.
  text_splitter:
    chunk_size: 1024
    chunk_overlap: 256

# -----------------------------------------------------------------------------
# Vector Store Settings (ChromaDB)
# -----------------------------------------------------------------------------
vector_store:
  # The name of the collection within ChromaDB.
  collection_name: "blog_agent_collection"

  # Directory to persist ChromaDB data, relative to the project root.
  persist_directory: "db/chroma"

  # The type of search to perform. Options: "similarity", "mmr", etc.
  search_type: "similarity"

  # Arguments for the search. `k` is the number of documents to retrieve.
  search_kwargs:
    k: 5

# -----------------------------------------------------------------------------
# Agent Settings
# -----------------------------------------------------------------------------
agent:
  # Configuration for the Tavily Search tool.
  tavily:
    max_results: 3

  # Configuration for the retriever tool.
  retriever_tool:
    name: "document_search"
    description: "Searches for relevant information within the uploaded documents."

# -----------------------------------------------------------------------------
# Redis Cache Settings
# -----------------------------------------------------------------------------
redis:
  host: "localhost"
  port: 6379

# -----------------------------------------------------------------------------
# Default Fallback Settings
# - Used if specific configurations are missing from the sections above.
# -----------------------------------------------------------------------------
defaults:
  text_splitter:
    chunk_size: 1000
    chunk_overlap: 200
  vector_store:
    collection_name: "default_collection"
    search_type: "similarity"
    search_kwargs:
      k: 3
  agent:
    tavily:
      max_results: 3
    retriever_tool:
      name: "document_search"
      description: "Document retriever tool"

# -------------------------------------------------------------------------------
# 8) 개발 참고사항 (Development Notes)
#    - 필요한 의존성 및 빠른 디버그 커맨드 (한글 주석)
# -------------------------------------------------------------------------------
# 필요한 의존성 (pyproject.toml에 추가 권장):
# - pyyaml
# - pymupdf
# - langchain-huggingface
# - sentence-transformers
# - langchain-upstage
#
# 개발용 설치 예:
# poetry install --extras dev
#
# 빠른 디버그 명령어 예시:
# 
# Streamlit
# pkill -f 'streamlit run' || true; sleep 1; poetry run streamlit run src/app.py --logger.level=debug
# 
# Chainlit
# pkill -f 'chainlit_app run' || true; sleep 1; poetry run chainlit run src/chainlit_app.py -w
# 
# To start Redis with the module, use:
# redis-server --loadmodule /home/wb2x/workspace/upstageailab-langchain-pjt-langchain_8/modules/rejson.so
#
# ENV
# poetry env activate
# 
# Pytest verbose mode example
# poetry run pytest -q tests/test_chainlit_edit_callbacks.py -q -rP
#  pkill -f 'chainlit' || true; sleep 1; poetry run chainlit run src/chainlit_app.py -w &> /tmp/chainlit_run.log & echo $! 
# 
# Tests
# poetry run python tests/test_vision.py
# poetry run python tests/test_audio.py
