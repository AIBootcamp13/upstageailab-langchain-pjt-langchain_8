# configs/config.yaml

# --- 환경 프로필 (Environment Profiles) ---
# ENV_PROFILE 환경 변수를 통해 활성 프로필을 선택합니다. (예: ENV_PROFILE=high_gpu)
# 이 프로필은 개발 환경(CPU vs GPU)에 따라 다른 모델 설정을 불러옵니다.
profiles:
  # 기본 프로필: 고성능 GPU가 없는 환경을 위한 설정
  default_cpu:
    # OpenAI API를 사용하는 API 기반 모델
    embedding_provider: "openai"
    embedding_model: "text-embedding-3-large"
    llm_provider: "openai"
    llm_model: "gpt-5-nano"

  # 고성능 프로필: 강력한 GPU가 있는 환경을 위한 설정
  high_gpu:
    # 로컬에서 실행되는 오픈소스 모델
    embedding_provider: "huggingface"
    embedding_model: "BAAI/bge-m3" # HuggingFace 로컬 모델
    llm_provider: "ollama"
    llm_model: "gpt-oss:20b" # Ollama 로컬 모델 (gpt-oss:20b,gemma3:4b,etc)

# --- 데이터 수집 (Ingestion) 설정 ---
ingestion:
  # PDF 파서(parser) 선택: "local" 또는 "api" 또는 "unstructured"
  parser: "local"

  # Text Splitter Settings ---
  text_splitter:
    chunk_size: 1024
    chunk_overlap: 256

  # Upstage API 로더를 위한 상세 설정
  api_loader:
    split: "page"
    output_format: "markdown"

# --- 벡터 저장소 (Vector Store) 설정 ---
vector_store:
  collection_name: "lecture_documents"
  # 검색 타입: "mmr" (Maximal Marginal Relevance) 또는 "similarity"
  search_type: "mmr"
  # 검색 관련 인자 (k: 반환할 문서 수)
  search_kwargs:
    k: 5


# --- ADD DEPENDENCIES ---
# In your pyproject.toml, add the following:
# "pyyaml"
# "pymupdf"
# "langchain-huggingface"
# "sentence-transformers"
# "langchain-upstage"